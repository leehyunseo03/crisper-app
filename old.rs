#![cfg_attr(not(debug_assertions), windows_subsystem = "windows")]

use tauri::Manager;
use tauri_plugin_shell::ShellExt;
use tokio::sync::Mutex;
use std::path::Path;
use std::fs;
use std::env;

// --- [ìˆ˜ì • 1] ëª…ì‹œì ì¸ ì„í¬íŠ¸ (ì—ëŸ¬ í•´ê²°ì˜ í•µì‹¬) ---
use rig::providers::openai::{self, Client};
use rig::vector_store::in_memory_store::InMemoryVectorStore;
use rig::embeddings::EmbeddingsBuilder;
use rig::completion::Prompt; // .prompt() ë©”ì„œë“œ ì‚¬ìš©ì„ ìœ„í•´ í•„ìˆ˜
use rig::Embed; // #[derive(Embed)] ì‚¬ìš©ì„ ìœ„í•´ í•„ìˆ˜
use rig::vector_store::VectorStoreIndex;

use serde::{Serialize, Deserialize};
use pdf_extract::extract_text;
use anyhow::Context;
use dotenvy::dotenv;

// ---------------------------------------------------------
// 1. ë°ì´í„° êµ¬ì¡°ì²´ ì •ì˜
// ---------------------------------------------------------
// [ìˆ˜ì • 2] Default ì¶”ê°€: InMemoryVectorStore::default() ì‚¬ìš©ì„ ìœ„í•´ í•„ìš”
#[derive(Embed, Clone, Debug, Serialize, Deserialize, Eq, PartialEq, Default)]
struct Document {
    id: String,
    name: String,
    #[embed]
    content: String,
}

// ---------------------------------------------------------
// 2. AppState ì •ì˜
// ---------------------------------------------------------
struct AppState {
    vector_store: Mutex<InMemoryVectorStore<Document>>,
    openai_client: Client,
}

// ---------------------------------------------------------
// 3. í—¬í¼ í•¨ìˆ˜
// ---------------------------------------------------------
fn load_pdf_content<P: AsRef<Path>>(file_path: P) -> anyhow::Result<String> {
    extract_text(file_path.as_ref())
        .with_context(|| format!("Failed to extract text from PDF: {:?}", file_path.as_ref()))
}

// [ì¶”ê°€ë¨] í…ìŠ¤íŠ¸ ì²­í‚¹ í•¨ìˆ˜ (Chunking)
// text: ì „ì²´ í…ìŠ¤íŠ¸
// chunk_size: ìë¥¼ ê¸€ì ìˆ˜ (ì˜ˆ: 2000)
// overlap: ê²¹ì¹  ê¸€ì ìˆ˜ (ì˜ˆ: 200 - ë¬¸ë§¥ ëŠê¹€ ë°©ì§€)
fn chunk_text(text: &str, chunk_size: usize, overlap: usize) -> Vec<String> {
    let chars: Vec<char> = text.chars().collect();
    let mut chunks = Vec::new();
    let mut start = 0;

    while start < chars.len() {
        let end = std::cmp::min(start + chunk_size, chars.len());
        let chunk: String = chars[start..end].iter().collect();
        
        // ë„ˆë¬´ ì§§ì€ ì²­í¬(ì˜ˆ: ê³µë°±ë§Œ ë‚¨ì€ ê²½ìš°)ëŠ” ë¬´ì‹œ
        if !chunk.trim().is_empty() {
            chunks.push(chunk);
        }

        // ëì— ë„ë‹¬í–ˆìœ¼ë©´ ì¢…ë£Œ
        if end == chars.len() {
            break;
        }

        // ë‹¤ìŒ ì‹œì‘ì  ê³„ì‚° (overlap ë§Œí¼ ë’¤ë¡œ ë‹¹ê²¨ì„œ ì‹œì‘) 
        start += chunk_size - overlap;
    }

    chunks
}
// ---------------------------------------------------------
// Command: PDF ì²˜ë¦¬ ë° ì„ë² ë”©
// ---------------------------------------------------------
#[tauri::command]
async fn process_pdfs(
    path: String,
    state: tauri::State<'_, AppState>,
) -> Result<String, String> {
    println!("ğŸ“‚ PDF ì²˜ë¦¬ ì‹œì‘ ê²½ë¡œ: {}", path);

    let directory_path = Path::new(&path);
    let entries = fs::read_dir(directory_path).map_err(|e| e.to_string())?;

    let embedding_model = state.openai_client.embedding_model("text-embedding-3-small");

    let mut docs: Vec<Document> = Vec::new();

    let chunk_size = 2000;
    let overlap = 200;

    for entry in entries {
        let entry = entry.map_err(|e| e.to_string())?;
        let file_path = entry.path();

        if file_path.extension().and_then(|s| s.to_str()) == Some("pdf") {
            let file_name = file_path.file_name().unwrap_or_default().to_string_lossy().to_string();

            if let Ok(content) = load_pdf_content(&file_path) {
                if !content.trim().is_empty() {
                    // [ë³€ê²½] ì „ì²´ ë‚´ìš©ì„ í•œë²ˆì— ë„£ëŠ”ê²Œ ì•„ë‹ˆë¼, ì²­í‚¹í•´ì„œ ì—¬ëŸ¬ ê°œë¡œ ë„£ìŠµë‹ˆë‹¤.
                    let chunks = chunk_text(&content, chunk_size, overlap);
                    
                    for (i, chunk) in chunks.into_iter().enumerate() {
                        docs.push(Document {
                            // IDë¥¼ ìœ ë‹ˆí¬í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ íŒŒì¼ëª… + ë²ˆí˜¸ë¥¼ ë¶™ì…ë‹ˆë‹¤.
                            id: format!("{}_part_{}", file_name, i), 
                            name: file_name.clone(),
                            content: chunk,
                        });
                    }
                    println!("âœ… ë¡œë“œ ë° ì²­í‚¹ ì™„ë£Œ: {} ({}ê°œì˜ ì¡°ê°)", file_name, docs.len());
                }
            }
        }
    }

    if docs.is_empty() {
        return Err("ì²˜ë¦¬í•  PDFê°€ ì—†ê±°ë‚˜ ë‚´ìš©ì„ ì½ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.".into());
    }
    let total_chunks = docs.len();

    println!("ğŸš€ {}ê°œì˜ ì²­í¬ì— ëŒ€í•´ ì„ë² ë”© ìƒì„± ì‹œì‘...", total_chunks);

    // ì„ë² ë”© ìƒì„±
    let embeddings = EmbeddingsBuilder::new(embedding_model.clone())
        .documents(docs)
        .map_err(|e| e.to_string())?
        .build()
        .await
        .map_err(|e| e.to_string())?;

    // ë²¡í„° ìŠ¤í† ì–´ì— ì¶”ê°€
    let mut store = state.vector_store.lock().await;
    
    // [ìˆ˜ì • 3] .await ì œê±°
    // InMemoryVectorStoreì˜ add_documentsëŠ” ë™ê¸° í•¨ìˆ˜ì´ê±°ë‚˜ ì¦‰ì‹œ ì™„ë£Œë˜ë¯€ë¡œ awaitê°€ í•„ìš” ì—†ìŠµë‹ˆë‹¤.
    store.add_documents(embeddings); 

    Ok(format!("{}ê°œì˜ ì²­í¬ê°€ ì„±ê³µì ìœ¼ë¡œ í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.", total_chunks))
}
// ---------------------------------------------------------
// Command : ë¬¸ì„œ ê²€ìƒ‰ (Context Retrieval)
// ---------------------------------------------------------
// ì§ˆë¬¸ì„ ë°›ì•„ì„œ ë²¡í„° DBì—ì„œ ìœ ì‚¬í•œ í…ìŠ¤íŠ¸ ì¡°ê°ì„ ì°¾ì•„ ë¬¸ìì—´ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
#[tauri::command]
async fn search_docs(
    query: String,
    state: tauri::State<'_, AppState>,
) -> Result<String, String> {
    let store = state.vector_store.lock().await;
    let embedding_model = state.openai_client.embedding_model("text-embedding-3-small");
    
    // 1. ì¸ë±ìŠ¤ ìƒì„± (store ë³µì œ)
    let index = store.clone().index(embedding_model);

    // 2. ìƒìœ„ 3ê°œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
    let results = index.top_n::<Document>(&query, 3)
        .await
        .map_err(|e| e.to_string())?;

    // 3. í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œí•˜ì—¬ í•˜ë‚˜ì˜ ë¬¸ìì—´ë¡œ í•©ì¹¨
    // í˜•ì‹:
    // [ì°¸ê³ ë¬¸ì„œ: íŒŒì¼ëª…]
    // ë‚´ìš©...
    let mut context_string = String::new();
    for (score, _id, doc) in results {
        // ìœ ì‚¬ë„ê°€ ë„ˆë¬´ ë‚®ì€ê±´ ì œì™¸í•  ìˆ˜ë„ ìˆìŒ (ì˜ˆ: score < 0.7)
        println!("{}",&format!("\n[ì°¸ê³ ë¬¸ì„œ: {} (ìœ ì‚¬ë„: {:.2})]\n{}\n", doc.name, score, doc.content));
        if score > 0.0 { 
            context_string.push_str(&format!("\n[ì°¸ê³ ë¬¸ì„œ: {} (ìœ ì‚¬ë„: {:.2})]\n{}\n", doc.name, score, doc.content));
        }
    }

    if context_string.is_empty() {
        return Ok("ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.".to_string());
    }

    Ok(context_string)
}

// ---------------------------------------------------------
// Command : ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
// ---------------------------------------------------------
#[tauri::command]
async fn download_model(app_handle: tauri::AppHandle, url: String, filename: String) -> Result<String, String> {
    eprintln!("ğŸš€ ë‹¤ìš´ë¡œë“œ ìš”ì²­ ìˆ˜ì‹ : {} -> {}", url, filename);
    
    // ëª¨ë¸ì´ ì €ì¥ë  í´ë” ê²½ë¡œ (src-tauri/models)
    let model_dir = app_handle.path().resource_dir().unwrap().join("models");
    
    // í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
    if !model_dir.exists() {
        std::fs::create_dir_all(&model_dir).map_err(|e| e.to_string())?;
    }

    // ì—¬ê¸°ì— ì‹¤ì œ ë‹¤ìš´ë¡œë“œ ë¡œì§ì´ ë“¤ì–´ê°‘ë‹ˆë‹¤. (í˜„ì¬ëŠ” ì„±ê³µ ë©”ì‹œì§€ë§Œ ë°˜í™˜)
    // ì‹¤ì œ êµ¬í˜„ì€ reqwest ë“±ì˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ê²Œ ë©ë‹ˆë‹¤.
    
    Ok(format!("{} ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì¤€ë¹„ ì™„ë£Œ (ê²½ë¡œ: {:?})", filename, model_dir))
}

// ---------------------------------------------------------
// Command: RAG ì±„íŒ…
// ---------------------------------------------------------
#[tauri::command]
async fn chat_with_docs(
    question: String,
    state: tauri::State<'_, AppState>,
) -> Result<String, String> {
    let store = state.vector_store.lock().await;
    
    let embedding_model = state.openai_client.embedding_model("text-embedding-3-small");

    let index = store.clone().index(embedding_model);

    let rag_agent = state.openai_client.agent("gpt-4o") 
        .preamble("You are a helpful assistant answering questions based on the provided PDF documents.")
        .dynamic_context(2, index)
        .build();

    // Prompt íŠ¸ë ˆì´íŠ¸ê°€ ì„í¬íŠ¸ë˜ì–´ ìˆì–´ì•¼ ì´ ë©”ì„œë“œê°€ ì‘ë™í•©ë‹ˆë‹¤.
    let response = rag_agent.prompt(&question).await.map_err(|e| e.to_string())?;

    Ok(response)
}

// ---------------------------------------------------------
// Main
// ---------------------------------------------------------
fn main() {
    dotenv().ok();
    let openai_client = Client::from_env();
    let vector_store = InMemoryVectorStore::<Document>::default();
    let app_state = AppState {
        vector_store: Mutex::new(vector_store),
        openai_client,
    };

    tauri::Builder::default()
        .plugin(tauri_plugin_shell::init())
        .plugin(tauri_plugin_dialog::init())
        .manage(app_state)
        // search_docs í•¸ë“¤ëŸ¬ ì¶”ê°€
        .invoke_handler(tauri::generate_handler![process_pdfs, search_docs]) 
        .setup(|app| {
            // --- ì‚¬ìš©ì ì œê³µ ì‚¬ì´ë“œì¹´ ë¡œì§ ---
            let resource_path = app.path().resource_dir().unwrap().join("binaries");
            let mut path_env = env::var_os("PATH").unwrap_or_default();
            let mut paths = env::split_paths(&path_env).collect::<Vec<_>>();
            paths.push(resource_path.clone());
            let _ = env::join_paths(paths).unwrap(); // new_path_env (ì‚¬ìš© ì•ˆí•¨ ê²½ê³  ë°©ì§€ ìœ„í•´ _ ì²˜ë¦¬)

            // ëª¨ë¸ ê²½ë¡œëŠ” ì‹¤ì œ ë°°í¬ì‹œ resource_path ë“±ì„ í™œìš©í•˜ëŠ”ê²Œ ì¢‹ìŠµë‹ˆë‹¤.
            // í˜„ì¬ëŠ” í•˜ë“œì½”ë”©ëœ ê²½ë¡œ ìœ ì§€
            let model_path = "C:/eoraha/crisper_app/crisper-app/src-tauri/models/ggml-model-Q4_K_M.gguf";

            let sidecar_command = app.shell().sidecar("llama-server").unwrap()
                .current_dir(resource_path)
                .args([
                    "--model", model_path,
                    "--port", "8080",
                    "--host", "127.0.0.1",
                    "--ctx-size", "4096", // RAGë¥¼ ìœ„í•´ ì»¨í…ìŠ¤íŠ¸ ì‚¬ì´ì¦ˆ ë„‰ë„‰í•˜ê²Œ
                    "--parallel", "1",
                    "--n-gpu-layers", "99" // GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë‹¤ë©´ ì¶”ê°€
                ]);

            let (mut rx, _) = sidecar_command.spawn().expect("ì‚¬ì´ë“œì¹´ ì‹¤í–‰ ì‹¤íŒ¨");

            tauri::async_runtime::spawn(async move {
                while let Some(event) = rx.recv().await {
                    if let tauri_plugin_shell::process::CommandEvent::Stderr(line) = event {
                         if let Ok(text) = String::from_utf8(line) {
                             // ë¡œê·¸ê°€ ë„ˆë¬´ ë§ìœ¼ë©´ ì£¼ì„ ì²˜ë¦¬ í•˜ì„¸ìš”
                             println!("LLAMA: {}", text.trim());
                         }
                    }
                }
            });
            Ok(())
        })
        .run(tauri::generate_context!())
        .expect("ì•± ì‹¤í–‰ ì˜¤ë¥˜");
}