// src-tauri/src/llm/extractor.rs
use crate::models::{LlmExtractionResult, LlmEntity, LlmRelation};
use serde_json::{json, Value};
use std::error::Error;
use reqwest::{Client, Response};

// ì§ì ‘ HTTP ìš”ì²­ì„ ë³´ë‚´ê¸° ìœ„í•´ reqwest ì‚¬ìš©
pub async fn extract_knowledge(
    base_url: &str, 
    text: &str
) -> Result<LlmExtractionResult, Box<dyn Error>> {

    let client = reqwest::Client::new();
    
    // main.rsì—ì„œ ì„¤ì •í•œ alias "gpt-3.5-turbo"ë¥¼ ì‚¬ìš©í•´ì•¼ llama-serverê°€ ì¸ì‹í•©ë‹ˆë‹¤.
    let model_name = "gpt-3.5-turbo"; 

    let system_instruction = r#"
    You are a Knowledge Graph Extractor.
    Extract entities and relationships from the text into JSON.
    RULES:
    1. Output ONLY valid JSON.
    2. Extract entities (Person, Topic, Tech, Event).
    3. Extract relations (actions, descriptions).
    4. If text is Korean, you can use Korean values.
    JSON SCHEMA:
    {
      "entities": [{"name": "...", "category": "...", "summary": "..."}],
      "relations": [{"head": "...", "relation": "...", "tail": "...", "reason": "..."}]
    }
    "#;

    // 1. ìš”ì²­ í˜ì´ë¡œë“œ êµ¬ì„± (OpenAI API í¬ë§·)
    let payload = json!({
        "model": model_name,
        "messages": [
            { "role": "system", "content": system_instruction },
            { "role": "user", "content": text }
        ],
        "temperature": 0.0, // ì •ë³´ ì¶”ì¶œì´ë¯€ë¡œ ì°½ì˜ì„± ì œí•œ
        "max_tokens": 4096,
        "stream": false,
        // llama-server ìµœì‹  ë²„ì „ì€ json_object ëª¨ë“œë¥¼ ì§€ì›í•˜ë¯€ë¡œ íŒíŠ¸ë¥¼ ì¤ë‹ˆë‹¤.
        "response_format": { "type": "json_object" } 
    });

    // 2. URL ì •ë¦¬ (ëì— ìŠ¬ë˜ì‹œ ì œê±° ë° ê²½ë¡œ ê²°í•©)
    // main.rsë‚˜ ingest.rsì—ì„œ "http://127.0.0.1:8081/v1" í˜•íƒœë¡œ ë“¤ì–´ì˜¨ë‹¤ê³  ê°€ì •
    let endpoint = format!("{}/chat/completions", base_url.trim_end_matches('/'));

    println!("ğŸ¤– [Extractor] Requesting to: {}", endpoint);

    // 3. POST ìš”ì²­ ì „ì†¡
    let res:Response = client.post(&endpoint)
        .json(&payload)
        .send()
        .await?;

    if !res.status().is_success() {
        let err_text = res.text().await?;
        return Err(format!("LLM Server Error: {}", err_text).into());
    }

    // 4. ì‘ë‹µ íŒŒì‹±
    let resp_json: Value = res.json().await?;
    
    // OpenAI í¬ë§·: choices[0].message.content
    let content = resp_json["choices"][0]["message"]["content"]
        .as_str()
        .ok_or("No content in response")?;

    // 5. í›„ì²˜ë¦¬ ë° JSON ë³€í™˜
    let cleaned = clean_json_response(content);
    
    // ë””ë²„ê¹…ìš© ë¡œê·¸ (í•„ìš”ì‹œ ì£¼ì„ ì²˜ë¦¬)
    // println!("ğŸ” Raw LLM Response: {}", cleaned);

    match serde_json::from_str::<LlmExtractionResult>(&cleaned) {
        Ok(result) => Ok(result),
        Err(e) => {
            println!("âŒ JSON Parsing Failed. Raw Content:\n{}", content);
            Err(Box::new(e))
        }
    }
}

// ë§ˆí¬ë‹¤ìš´ ì½”ë“œ ë¸”ë¡ ì œê±° í—¬í¼ í•¨ìˆ˜
fn clean_json_response(response: &str) -> String {
    let mut clean = response.trim().to_string();
    
    // ```json ... ``` ì œê±° ë¡œì§
    if let Some(start) = clean.find("```json") { 
        clean = clean[start+7..].to_string(); 
    } else if let Some(start) = clean.find("```") { 
        clean = clean[start+3..].to_string(); 
    }
    
    if let Some(end) = clean.rfind("```") { 
        clean = clean[..end].to_string(); 
    }
    
    clean.trim().to_string()
}