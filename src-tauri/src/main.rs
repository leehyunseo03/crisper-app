// src-tauri/src/main.rs
#![cfg_attr(not(debug_assertions), windows_subsystem = "windows")]

mod models;
mod database;
mod utils;
mod llm;
mod commands;

use tauri::{Manager, RunEvent, AppHandle, Emitter};
use tauri_plugin_shell::ShellExt;
use tauri_plugin_shell::process::{CommandChild, CommandEvent}; // CommandEvent ì¶”ê°€
use std::env;
use std::sync::{Arc, Mutex};
use std::time::Duration; // ë”œë ˆì´ìš©
use tokio::time::sleep;  // ë¹„ë™ê¸° ë”œë ˆì´
use rig::providers::openai::Client as OpenAiClient;
use surrealdb::engine::local::{Db, RocksDb};
use surrealdb::Surreal;

// AppState êµ¬ì¡°ì²´
struct AppState {
    db: Surreal<Db>,
    embed_client: OpenAiClient, // Port 8080
    gen_client: OpenAiClient,   // Port 8081
    server_handles: Arc<Mutex<Vec<CommandChild>>>,
}

// â™»ï¸ ì„œë²„ ì‹¤í–‰/ì¬ì‹œì‘ì„ ë‹´ë‹¹í•˜ëŠ” í•µì‹¬ í•¨ìˆ˜
async fn start_servers(app: &AppHandle, use_gpu: bool) {
    let state = app.state::<AppState>();
    
    // 1. ê¸°ì¡´ í”„ë¡œì„¸ìŠ¤ ì£½ì´ê¸° (Clean up)
    {
        let mut handles = state.server_handles.lock().unwrap();
        if !handles.is_empty() {
            println!("ğŸ›‘ ê¸°ì¡´ ì„œë²„ ì¢…ë£Œ ì¤‘...");
            for child in handles.drain(..) {
                let _ = child.kill();
            }
        }
    }
    // í¬íŠ¸ ë°˜í™˜ ëŒ€ê¸° (ì•ˆì „ì¥ì¹˜)
    sleep(Duration::from_secs(2)).await;

    // 2. ê²½ë¡œ ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì •
    let resource_path = app.path().resource_dir().unwrap().join("binaries");
    let path_env = env::var_os("PATH").unwrap_or_default();
    let mut paths = env::split_paths(&path_env).collect::<Vec<_>>();
    paths.push(resource_path.clone());
    let new_path_env = env::join_paths(paths).unwrap();

    // ğŸš¨ ëª¨ë¸ ê²½ë¡œ (ë³¸ì¸ ê²½ë¡œë¡œ í™•ì¸ í•„ìˆ˜!)
    let embed_model_path = "C:/eoraha/crisper_app/crisper-app/src-tauri/models/ggml-model-Q4_K_M.gguf";
    let chat_model_path  = "C:/eoraha/crisper_app/crisper-app/src-tauri/models/qwen2.5-7b-instruct-q2_k.gguf";

    // 3. GPU ì˜µì…˜ ê²°ì •
    // GPU ëª¨ë“œë©´ 99ë ˆì´ì–´(ì „ë¶€), CPU ëª¨ë“œë©´ 0ë ˆì´ì–´
    let embed_gpu = if use_gpu { "99" } else { "0" };
    let chat_gpu  = if use_gpu { "10" } else { "0" }; // ì±„íŒ…ì€ VRAM ë¶€ì¡± ë°©ì§€ë¡œ 10ë§Œ

    println!("ğŸš€ ì„œë²„ ì‹œì‘ (GPU ëª¨ë“œ: {})", use_gpu);

    // 4. ì„ë² ë”© ì„œë²„ (8080) ì‹¤í–‰
    let (mut rx1, child1) = app.shell().sidecar("llama-server").unwrap()
        .current_dir(&resource_path)
        .env("PATH", &new_path_env)
        .args([
            "--model", embed_model_path,
            "--port", "8080", "--host", "127.0.0.1",
            "--embedding", "--pooling", "mean",
            "--ctx-size", "2048", "--batch-size", "2048", "--ubatch-size", "2048",
            "--parallel", "1",
            "--n-gpu-layers", embed_gpu // ğŸ‘ˆ ë™ì  í• ë‹¹
        ])
        .spawn().expect("8080 ì„œë²„ ì‹¤íŒ¨");

    state.server_handles.lock().unwrap().push(child1);

    // 5. ì±„íŒ… ì„œë²„ (8081) ì‹¤í–‰
    let (mut rx2, child2) = app.shell().sidecar("llama-server").unwrap()
        .current_dir(&resource_path)
        .env("PATH", &new_path_env)
        .args([
            "--model", chat_model_path,
            "--alias", "gpt-3.5-turbo",
            "--port", "8081", 
            "--host", "127.0.0.1",
            //"--api", "openai",
            "--ctx-size", "4096", "--batch-size", "2048", "--ubatch-size", "2048",
            "--parallel", "2",
            "--n-gpu-layers", chat_gpu // ğŸ‘ˆ ë™ì  í• ë‹¹
        ])
        .spawn().expect("8081 ì„œë²„ ì‹¤íŒ¨");

    state.server_handles.lock().unwrap().push(child2);

    // (ì„ íƒ) ë¡œê·¸ ëª¨ë‹ˆí„°ë§ì€ ì—¬ê¸°ì„œ ê°„ë‹¨íˆ ì²˜ë¦¬í•˜ê±°ë‚˜ ìƒëµ ê°€ëŠ¥
    // ...
    println!("ğŸš€ ì ìš© ì™„ë£Œ! (GPU ëª¨ë“œ: {})", use_gpu);
}

// ğŸ›ï¸ í”„ë¡ íŠ¸ì—”ë“œì—ì„œ í˜¸ì¶œí•  í† ê¸€ ì»¤ë§¨ë“œ
#[tauri::command]
async fn toggle_gpu(app: AppHandle, enable: bool) -> Result<String, String> {
    println!("ğŸ›ï¸ GPU í† ê¸€ ìš”ì²­: {}", enable);
    start_servers(&app, enable).await;
    Ok(if enable { "GPU Mode ON" } else { "CPU Mode ON" }.to_string())
}

#[tokio::main]
async fn main() {
    dotenvy::dotenv().ok();

    if std::env::var("RUST_LOG").is_err() {
        std::env::set_var("RUST_LOG", "reqwest=trace"); // reqwestì˜ ë””ë²„ê·¸ ë¡œê·¸ë§Œ ë´…ë‹ˆë‹¤
    }
    env_logger::init();

    let db = database::init_db().await.expect("DB Init Failed");
    let embed_client = OpenAiClient::builder().base_url("http://127.0.0.1:8080/v1").api_key("sk-no-key").build().unwrap();
    let gen_client = OpenAiClient::builder().base_url("http://127.0.0.1:8081/v1").api_key("sk-no-key").build().unwrap();
    
    // í•¸ë“¤ ì €ì¥ì†Œ ìƒì„±
    let server_handles = Arc::new(Mutex::new(Vec::new()));

    let app_state = AppState { 
        db, embed_client, gen_client, 
        server_handles: server_handles.clone() 
    };

    let app = tauri::Builder::default()
        .plugin(tauri_plugin_shell::init())
        .plugin(tauri_plugin_dialog::init())
        .manage(app_state)
        .invoke_handler(tauri::generate_handler![
            crate::commands::ingest::process_pdfs,
            crate::commands::query::fetch_graph_data,
            toggle_gpu, // ğŸ‘ˆ ì»¤ë§¨ë“œ ë“±ë¡!
        ])
        .setup(move |app| {
            // ì•± ì¼œì§ˆ ë•ŒëŠ” ê¸°ë³¸ì ìœ¼ë¡œ CPU ëª¨ë“œ(false)ë¡œ ì‹œì‘ (í˜¹ì€ trueë¡œ ì„¤ì • ê°€ëŠ¥)
            let handle = app.handle().clone();
            tauri::async_runtime::spawn(async move {
                start_servers(&handle, false).await; 
            });
            Ok(())
        })
        .build(tauri::generate_context!())
        .expect("Error building app");

    app.run(move |_app_handle, event| {
        if let RunEvent::Exit = event {
            // ì¢…ë£Œ ì‹œ ì •ë¦¬
            let mut guards = server_handles.lock().unwrap();
            for child in guards.drain(..) { let _ = child.kill(); }
        }
    });
}
