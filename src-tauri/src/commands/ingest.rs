// src-tauri/src/commands/ingest.rs
use tauri::State;
use std::path::Path;
use std::fs;
use uuid::Uuid;
use chrono::Utc;
use surrealdb::engine::local::Db;
use surrealdb::Surreal;
use surrealdb::sql::Thing;
use rig::embeddings::EmbeddingsBuilder;
use rig::client::EmbeddingsClient;
use std::collections::HashMap;
use serde_json::json;
use std::time::Instant;
use serde::{Serialize, Deserialize};

use crate::models::{EventNode, DocumentNode, ChunkNode, EntityNode, LlmExtractionResult};
use crate::utils::sanitize_id;
use crate::utils::{extract_pages_from_pdf, chunk_text, RigDoc};
use crate::llm::extractor::{extract_knowledge, summarize_document};
use crate::AppState;
use crate::utils::parse_kakao_talk_log;

#[derive(Debug, Serialize, Deserialize)]
pub struct DocumentWithChunks {
    pub id: Thing,
    pub filename: String,
    pub created_at: chrono::DateTime<Utc>,
    pub metadata: HashMap<String, serde_json::Value>,
    // ğŸŒŸ ì—¬ê¸°ê°€ í•µì‹¬: SurrealDBê°€ ì—°ê²°ëœ ì²­í¬ë“¤ì„ ì´ í•„ë“œì— ì±„ì›Œì¤ë‹ˆë‹¤.
    #[serde(default)] 
    pub chunks: Vec<ChunkNode>, 
}

#[tauri::command]
pub async fn ingest_documents(
    path: String,
    state: State<'_, AppState>,
) -> Result<String, String> {
    let db = &state.db;
    let gen_url = "http://127.0.0.1:8081/v1";

    println!("\nğŸ“‚ [Step 1] Ingest Process Started (1 Page = 1 Chunk)");
    println!("   Target Directory: {}", path);

    // 1. íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ (ê¸°ì¡´ ë™ì¼)
    let entries = fs::read_dir(&path).map_err(|e| e.to_string())?;
    let mut pdf_files = Vec::new();
    for entry in entries { /* ... */ 
        let entry = entry.map_err(|e| e.to_string())?;
        let path = entry.path();
        if path.extension().and_then(|s| s.to_str()) == Some("pdf") {
            pdf_files.push(path);
        }
    }
    
    let total_files = pdf_files.len();
    if total_files == 0 { return Err("No PDF files found.".to_string()); }

    // 2. ì„¸ì…˜ ìƒì„± (ê¸°ì¡´ ë™ì¼)
    let session_id = Uuid::new_v4().to_string();
    let _: EventNode = db.create(("event", &session_id))
        .content(EventNode {
            id: None, summary: format!("PDF Ingest: {}", path), created_at: Utc::now(),
        }).await.map_err(|e| e.to_string())?.ok_or("Event create failed")?;

    let mut success_count = 0;

    // 3. íŒŒì¼ ì²˜ë¦¬ ë£¨í”„
    for (idx, file_path) in pdf_files.iter().enumerate() {
        let current_num = idx + 1;
        let original_filename = file_path.file_name().unwrap().to_string_lossy().to_string();
        
        println!("\n---------------------------------------------------");
        println!("â–¶ï¸  [{}/{}] Processing: {}", current_num, total_files, original_filename);
        let file_start = Instant::now();

        // A. ğŸŒŸ [í•µì‹¬ ë³€ê²½] í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Vec<String>)
        print!("    ğŸ“– Extracting pages... ");
        let pages = match extract_pages_from_pdf(file_path) {
            Ok(p) => {
                println!("Done ({} pages)", p.len());
                p
            },
            Err(e) => {
                println!("âŒ Failed: {}", e);
                continue;
            }
        };

        if pages.is_empty() { 
            println!("    âš ï¸ Skipped (Empty PDF)");
            continue; 
        }

        // B. Document(ë¶€ëª¨) ìš”ì•½ ìƒì„±
        // ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ, ì•ìª½ 1~2í˜ì´ì§€ë¥¼ í•©ì³ì„œ ë¶€ëª¨ ë¬¸ì„œì˜ ìš”ì•½ìš©ìœ¼ë¡œ ì”ë‹ˆë‹¤.
        let summary_context = pages.iter().take(2).cloned().collect::<Vec<String>>().join("\n");
        
        println!("    ğŸ¤– Summarizing Document (Parent)...");
        let parent_summary = summarize_document(gen_url, &summary_context).await.unwrap_or_else(|_| {
             crate::llm::extractor::DocSummaryResult {
                title: original_filename.clone(), summary: "Parent Summary Failed".to_string(), tags: vec![]
            }
        });

        // Document ì €ì¥
        let doc_id = Uuid::new_v4().to_string();
        let mut doc_meta = HashMap::new();
        doc_meta.insert("title".to_string(), json!(parent_summary.title));
        doc_meta.insert("summary".to_string(), json!(parent_summary.summary));
        
        let _doc: DocumentNode = db.create(("document", &doc_id))
            .content(DocumentNode { 
                id: None, filename: original_filename.clone(), created_at: Utc::now(), metadata: doc_meta 
            }).await.map_err(|e| e.to_string())?.expect("Failed to create doc");

        // Event ì—°ê²°
        let _ = db.query("RELATE $e->imported->$d").bind(("e", session_id.clone())).bind(("d", format!("document:{}", doc_id))).await.ok();

        // C. ì²­í‚¹ (ì´ë¯¸ í˜ì´ì§€ë³„ë¡œ ë‚˜ëˆ ì ¸ ìˆìœ¼ë¯€ë¡œ chunk_text í•¨ìˆ˜ í˜¸ì¶œ ì•ˆ í•¨!)
        // let chunks = chunk_text(...) -> ì‚­ì œ!
        // pages ë³€ìˆ˜ ìì²´ê°€ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        let chunks = pages; 

        println!("    Process {} Pages as Chunks...", chunks.len());

        // D. ê° í˜ì´ì§€ë³„ LLM ìš”ì•½ ì‹¤í–‰
        for (i, txt) in chunks.iter().enumerate() {
            let chunk_uuid = Uuid::new_v4().to_string();
            
            // í˜ì´ì§€ê°€ ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë‹ˆ ìš”ì•½ìš©ìœ¼ë¡œëŠ” ì•ë¶€ë¶„ë§Œ ìë¥¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            // ì—¬ê¸°ì„  ê·¸ëŒ€ë¡œ ë„£ìŠµë‹ˆë‹¤.
            print!("       Running LLM on Page #{}... ", i + 1);
            
            // í˜ì´ì§€ë³„ ìš”ì•½ (ì œëª©ì— í˜ì´ì§€ ë²ˆí˜¸ ìë™ ë¶€ì—¬)
            let chunk_res = summarize_document(gen_url, txt).await.unwrap_or_else(|_| {
                 crate::llm::extractor::DocSummaryResult {
                    title: format!("Page {}", i+1), // LLM ì‹¤íŒ¨ì‹œ "Page 1" ë“±ìœ¼ë¡œ ì œëª© ì„¤ì •
                    summary: "ìš”ì•½ ì‹¤íŒ¨".to_string(),
                    tags: vec![]
                }
            });
            println!("Done");

            let mut chunk_meta = HashMap::new();
            chunk_meta.insert("title".to_string(), json!(chunk_res.title)); // "ì„œë¡ ", "ê²°ë¡ " ë“± í˜ì´ì§€ ë‚´ìš©ì„ ë°˜ì˜í•œ ì œëª©
            chunk_meta.insert("summary".to_string(), json!(chunk_res.summary));
            chunk_meta.insert("tags".to_string(), json!(chunk_res.tags));
            chunk_meta.insert("page_number".to_string(), json!(i + 1)); // ğŸŒŸ ëª‡ í˜ì´ì§€ì¸ì§€ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€

            // Chunk ì €ì¥
            let _chunk: ChunkNode = db.create(("chunk", &chunk_uuid))
                .content(ChunkNode {
                    id: None, 
                    content: txt.clone(), 
                    page_index: i, 
                    embedding: vec![],
                    metadata: chunk_meta 
                }).await.map_err(|e| e.to_string())?.expect("Chunk create failed");
            println!("       ----------------------------------------");
            println!("       ğŸ“„ Title:   {}", chunk_res.title);
            println!("       ğŸ“ Summary: {}", chunk_res.summary);
            println!("       ğŸ·ï¸ Tags:    {:?}", chunk_res.tags);
            println!("       ----------------------------------------");
            // Document -> Chunk ì—°ê²°
            let doc_thing = Thing::from(("document", doc_id.as_str()));
            let chunk_thing = Thing::from(("chunk", chunk_uuid.as_str()));

            db.query("RELATE $d->contains->$c")
                .bind(("d", doc_thing))
                .bind(("c", chunk_thing))
                .await
                .ok();
        }

        println!("    âœ¨ File completed in {:.2?}", file_start.elapsed());
        success_count += 1;
    }

    Ok(format!("âœ… Processed {} files.", success_count))
}
// --- 2ë‹¨ê³„: Document(Chunk) -> Graph (ì˜¤ë˜ ê±¸ë¦¼) ---
#[tauri::command]
pub async fn construct_graph(
    state: State<'_, AppState>,
) -> Result<String, String> {
    let db = &state.db;
    let gen_url = "http://127.0.0.1:8081/v1";

    println!("ğŸ•¸ï¸ [Step 2] Building Knowledge Graph...");

    // 1. ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€(mentions ê´€ê³„ê°€ ì—†ëŠ”) Chunkë“¤ì„ ì¡°íšŒ
    //    ì£¼ì˜: surrealql ë¬¸ë²•ì— ë”°ë¼ `count(->mentions) = 0` ì‚¬ìš©
    //    ì„±ëŠ¥ì„ ìœ„í•´ í•œ ë²ˆì— 10ê°œì”©ë§Œ ì²˜ë¦¬í•˜ê±°ë‚˜ ë£¨í”„ë¥¼ ë•ë‹ˆë‹¤. ì—¬ê¸°ì„  ì˜ˆì‹œë¡œ 20ê°œ ì œí•œ.
    let mut chunks_to_process: Vec<ChunkNode> = db.query("SELECT * FROM chunk WHERE count(->mentions) = 0 LIMIT 20")
        .await.map_err(|e| e.to_string())?
        .take(0).map_err(|e| e.to_string())?;

    if chunks_to_process.is_empty() {
        return Ok("âœ¨ ì²˜ë¦¬í•  ìƒˆë¡œìš´ Chunkê°€ ì—†ìŠµë‹ˆë‹¤.".to_string());
    }

    let total = chunks_to_process.len();
    println!("   ğŸš€ Processing {} chunks...", total);

    for (idx, chunk) in chunks_to_process.iter().enumerate() {
        let chunk_id_raw = chunk.id.as_ref().unwrap().id.to_string(); // thingì—ì„œ id ë¶€ë¶„ë§Œ ì¶”ì¶œ í•„ìš”í•  ìˆ˜ ìˆìŒ
        // SurrealDB Rust SDKì˜ Thing.idëŠ” Id íƒ€ì…ì´ë¯€ë¡œ to_string()í•˜ë©´ ê´„í˜¸ ë“±ì´ í¬í•¨ë  ìˆ˜ ìˆìŒ.
        // ì•ˆì „í•˜ê²Œ Thing ìì²´ë¥¼ ì‚¬ìš©í•˜ê±°ë‚˜ Stringìœ¼ë¡œ ë³€í™˜. ì—¬ê¸°ì„  String ë³€í™˜ ê°€ì •.
        
        // 2. LLM ì¶”ì¶œ
        match extract_knowledge(gen_url, &chunk.content).await {
            Ok(result) => {
                // 3. ê·¸ë˜í”„ ë°ì´í„° ì €ì¥ (ì¬ì‚¬ìš©)
                // chunk_id_rawê°€ "chunk:uuid" í˜•íƒœì¸ì§€ "uuid" í˜•íƒœì¸ì§€ í™•ì¸ í•„ìš”.
                // save_graph_dataëŠ” "uuid" ë¬¸ìì—´ì„ ê¸°ëŒ€í•˜ë„ë¡ ì‘ì„±ë˜ì—ˆìŒ.
                let simple_id = chunk_id_raw.replace("chunk:", "").replace("âŸ¨", "").replace("âŸ©", "");
                
                if let Err(e) = save_graph_data(db, simple_id, &result).await {
                    eprintln!("âŒ Save Error: {}", e);
                } else {
                    println!("   âœ… [{}/{}] Graph extracted for chunk", idx + 1, total);
                }
            },
            Err(e) => eprintln!("âŒ Extraction Error: {}", e),
        }
    }

    Ok(format!("âœ… {}ê°œ Chunkì— ëŒ€í•œ ê·¸ë˜í”„ ìƒì„± ì™„ë£Œ", total))
}



#[tauri::command]
pub async fn get_documents(state: State<'_, AppState>) -> Result<Vec<DocumentWithChunks>, String> {
    let db = &state.db;
    
    // ğŸŒŸ [ìˆ˜ì • í•µì‹¬] ì„œë¸Œì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•´ ì—°ê²°ëœ ë°ì´í„°ë¥¼ ì¤‘ì²© êµ¬ì¡°ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    // ì˜ë¯¸: "documentë¥¼ ê°€ì ¸ì˜¤ëŠ”ë°, 'chunks'ë¼ëŠ” í•„ë“œì—ëŠ” 
    //      ë‚˜(document)ì™€ 'contains'ë¡œ ì—°ê²°ëœ 'chunk'ë“¤ì„ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ë‹´ì•„ë¼"
    let sql = "
        SELECT 
            *, 
            (SELECT * FROM ->contains->chunk ORDER BY page_index ASC) AS chunks 
        FROM document 
        ORDER BY created_at DESC
    ";
    
    // ì¿¼ë¦¬ ì‹¤í–‰
    let mut response = db.query(sql).await.map_err(|e| e.to_string())?;
    
    // ê²°ê³¼ë¥¼ ìƒˆë¡œ ë§Œë“  êµ¬ì¡°ì²´(DocumentWithChunks) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    let documents: Vec<DocumentWithChunks> = response.take(0).map_err(|e| e.to_string())?;
    
    Ok(documents)
}

async fn save_graph_data(
    db: &Surreal<Db>,
    chunk_id: String,
    data: &LlmExtractionResult,
) -> Result<(), String> {
    
    // 1. Entity ì €ì¥ ë° Chunkì™€ ì—°ê²°
    for entity in &data.entities {
        let safe_name = sanitize_id(&entity.name);
        
        // ğŸŒŸ [ìˆ˜ì • í•µì‹¬] Rustì—ì„œ ì§ì ‘ Thing(ID) ê°ì²´ ìƒì„±
        let chunk_thing = Thing::from(("chunk", chunk_id.as_str()));
        let entity_thing = Thing::from(("entity", safe_name.as_str()));

        // 1-1. Entity ìƒì„±/ì—…ë°ì´íŠ¸
        let _: Option<EntityNode> = db
            .upsert(("entity", &safe_name))
            .content(EntityNode {
                id: None,
                name: entity.name.clone(),
                category: entity.category.clone(),
                description: entity.summary.clone(),
                embedding: vec![],
                created_at: Utc::now(),
            })
            .await
            .map_err(|e| format!("Entity Upsert Error: {}", e))?;

        // 1-2. Chunk -> Entity ì—°ê²° (SQLì´ í›¨ì”¬ ê¹”ë”í•´ì§‘ë‹ˆë‹¤)
        // ê¸°ì¡´: RELATE type::thing(...) -> ...
        // ë³€ê²½: RELATE $c -> mentions -> $e
        let sql = "RELATE $c -> mentions -> $e";
        
        let _ = db.query(sql)
            // ğŸŒŸ String ëŒ€ì‹  Thing ê°ì²´ë¥¼ ë°”ì¸ë”©í•©ë‹ˆë‹¤.
            // DBëŠ” ì´ê±¸ ë°›ì•„ì„œ "ì•„, ì´ê±´ ë¬¸ìì—´ì´ ì•„ë‹ˆë¼ ë ˆì½”ë“œ IDêµ¬ë‚˜"ë¼ê³  ë°”ë¡œ ì¸ì‹í•©ë‹ˆë‹¤.
            .bind(("c", chunk_thing)) 
            .bind(("e", entity_thing))
            .await
            .map_err(|e| format!("Relate Chunk-Entity Error: {}", e))?;
    }

    // 2. Relation (Entity -> Entity) ì €ì¥
    for rel in &data.relations {
        let head_safe = sanitize_id(&rel.head);
        let tail_safe = sanitize_id(&rel.tail);

        // ğŸŒŸ ì—¬ê¸°ë„ Thing ê°ì²´ ìƒì„±
        let head_thing = Thing::from(("entity", head_safe.as_str()));
        let tail_thing = Thing::from(("entity", tail_safe.as_str()));

        // Head/Tail ë…¸ë“œ ì´ë¦„ ë³´ì¥ (ë¹ˆ ê»ë°ê¸° ìƒì„±)
        let _ = db.query("UPDATE type::thing('entity', $id) SET name = $name RETURN NONE")
            .bind(("id", head_safe.clone()))
            .bind(("name", rel.head.clone()))
            .await;
            
        let _ = db.query("UPDATE type::thing('entity', $id) SET name = $name RETURN NONE")
            .bind(("id", tail_safe.clone()))
            .bind(("name", rel.tail.clone()))
            .await;

        // 2-1. ê´€ê³„ ìƒì„±
        let sql = "
            RELATE $h -> related_to -> $t
            CONTENT {
                relation: $rel,
                reason: $reason,
                created_at: time::now()
            }
        ";
        
        let _ = db.query(sql)
            .bind(("h", head_thing)) // Thing ê°ì²´ ë°”ì¸ë”©
            .bind(("t", tail_thing)) // Thing ê°ì²´ ë°”ì¸ë”©
            .bind(("rel", rel.relation.clone()))
            .bind(("reason", rel.reason.clone()))
            .await
            .map_err(|e| format!("Relate Entity-Entity Error: {}", e))?;
    }

    Ok(())
}