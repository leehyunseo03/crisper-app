use tauri::State;
use std::fs;
use uuid::Uuid;
use chrono::Utc;
use surrealdb::sql::Thing;
use std::collections::{HashMap, HashSet};
use serde_json::json;
use std::time::Instant;

use crate::models::{EventNode, DocumentNode, ChunkNode, EntityNode, DocumentWithChunks, CoreAnalysisResult};
use crate::utils::extract_pages_from_pdf;
use crate::llm::extractor::analyze_content;
use crate::AppState;

// --- 1ë‹¨ê³„: PDF íŒŒì¼ Ingest ë° êµ¬ì¡° ë¶„ì„ (LLM) ---
#[tauri::command]
pub async fn ingest_documents(
    path: String,
    state: State<'_, AppState>,
) -> Result<String, String> {
    let db = &state.db;
    let gen_url = "[http://127.0.0.1:8081/v1](http://127.0.0.1:8081/v1)"; // ë¡œì»¬ LLM ì„œë²„ ì£¼ì†Œ

    println!("\nğŸ“‚ [Step 1] Ingest Process Started (1 Page = 1 Chunk)");
    println!("    Target Directory: {}", path);

    // 1. íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    let entries = fs::read_dir(&path).map_err(|e| e.to_string())?;
    let mut pdf_files = Vec::new();
    for entry in entries {
        let entry = entry.map_err(|e| e.to_string())?;
        let path = entry.path();
        if path.extension().and_then(|s| s.to_str()) == Some("pdf") {
            pdf_files.push(path);
        }
    }
    
    let total_files = pdf_files.len();
    if total_files == 0 { return Err("No PDF files found.".to_string()); }

    // 2. ì„¸ì…˜ ìƒì„± (ì‘ì—… ê¸°ë¡ìš© Event)
    let session_id = Uuid::new_v4().to_string();
    let _: EventNode = db.create(("event", &session_id))
        .content(EventNode {
            id: None, summary: format!("PDF Ingest: {}", path), created_at: Utc::now(),
        }).await.map_err(|e| e.to_string())?.ok_or("Event create failed")?;

    let mut success_count = 0;

    // 3. íŒŒì¼ ì²˜ë¦¬ ë£¨í”„
    for (idx, file_path) in pdf_files.iter().enumerate() {
        let current_num = idx + 1;
        let original_filename = file_path.file_name().unwrap().to_string_lossy().to_string();
        
        println!("\n---------------------------------------------------");
        println!("â–¶ï¸  [{}/{}] Processing: {}", current_num, total_files, original_filename);
        
        // A. í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ
        print!("    ğŸ“– Extracting pages... ");
        let pages = match extract_pages_from_pdf(file_path) {
            Ok(p) => {
                println!("Done ({} pages)", p.len());
                p
            },
            Err(e) => {
                println!("âŒ Failed: {}", e);
                continue;
            }
        };

        if pages.is_empty() { 
            println!("    âš ï¸ Skipped (Empty PDF)");
            continue; 
        }

        // B. Document(ë¶€ëª¨) ìš”ì•½ ìƒì„± (ì• 2í˜ì´ì§€ë§Œ ì‚¬ìš©)
        let summary_context = pages.iter().take(2).cloned().collect::<Vec<String>>().join("\n");
        
        println!("    ğŸ¤– Summarizing Document (Parent)...");
        let parent_analysis = analyze_content(gen_url, &summary_context).await.unwrap_or_else(|_| {
             CoreAnalysisResult {
                topic: original_filename.clone(),
                summary: "ë¶„ì„ ì‹¤íŒ¨".to_string(),
                key_entities: vec![],
                detailed_data: json!({}),
            }
        });

        // Document ì €ì¥
        let doc_id = Uuid::new_v4().to_string();
        let mut doc_meta = HashMap::new();
        doc_meta.insert("analysis".to_string(), json!(parent_analysis));

        let _doc: DocumentNode = db.create(("document", &doc_id))
            .content(DocumentNode { 
                id: None, filename: original_filename.clone(), created_at: Utc::now(), metadata: doc_meta 
            }).await.map_err(|e| e.to_string())?.expect("Failed to create doc");

        // Event -> Document ì—°ê²°
        let _ = db.query("RELATE $e->imported->$d").bind(("e", session_id.clone())).bind(("d", format!("document:{}", doc_id))).await.ok();

        // C. ì²­í¬ ì²˜ë¦¬ (í˜ì´ì§€ ë‹¨ìœ„)
        let chunks = pages; 
        for (i, txt) in chunks.iter().enumerate() {
            let chunk_uuid = Uuid::new_v4().to_string();
            
            print!("      Running LLM Analysis on Page #{} (Len: {})... ", i + 1, txt.len());
            
            // í˜ì´ì§€ë³„ ë¶„ì„ ì‹¤í–‰
            let chunk_res = match analyze_content(gen_url, txt).await {
                Ok(res) => {
                    println!("âœ… Done");
                    res
                },
                Err(e) => {
                    println!("\n      âŒ ERROR: {:?}", e);
                    CoreAnalysisResult {
                        topic: format!("Page {}", i+1),
                        summary: "ë¶„ì„ ì‹¤íŒ¨".to_string(),
                        key_entities: vec![],
                        detailed_data: json!({ "error": format!("{:?}", e) }),
                    }
                }
            };

            // Chunk ë©”íƒ€ë°ì´í„° êµ¬ì„±
            let mut chunk_meta = HashMap::new();
            chunk_meta.insert("page_number".to_string(), json!(i + 1));
            // Step 2(Graph)ë¥¼ ìœ„í•´ ë¶„ì„ ë°ì´í„°ë¥¼ í†µì§¸ë¡œ ì €ì¥
            chunk_meta.insert("analysis".to_string(), json!(chunk_res)); 

            // Chunk ì €ì¥
            let _chunk: ChunkNode = db.create(("chunk", &chunk_uuid))
                .content(ChunkNode {
                    id: None, 
                    content: txt.clone(), 
                    page_index: i, 
                    embedding: vec![], // ì„ë² ë”©ì€ í•„ìš” ì‹œ ë‚˜ì¤‘ì— ì¶”ê°€
                    metadata: chunk_meta 
                }).await.map_err(|e| e.to_string())?.expect("Chunk create failed");

            // Document -> Chunk ì—°ê²°
            let _ = db.query("RELATE $d->contains->$c")
                .bind(("d", format!("document:{}", doc_id)))
                .bind(("c", format!("chunk:{}", chunk_uuid)))
                .await.ok();
        }
        success_count += 1;
    }
    
    Ok(format!("âœ… Processed {} files with Structural Analysis.", success_count))
}

// --- 2ë‹¨ê³„: Chunk ë©”íƒ€ë°ì´í„° -> í‚¤ì›Œë“œ Graph ì—°ê²° ---
#[tauri::command]
pub async fn construct_graph(
    state: State<'_, AppState>,
) -> Result<String, String> {
    let db = &state.db;

    println!("\nğŸ•¸ï¸ [Step 2] Building Keyword Graph (No LLM)...");

    // 1. ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ Chunk ì¡°íšŒ
    let sql = "SELECT * FROM chunk WHERE metadata.step2_processed != true LIMIT 500";
    
    let mut chunks_to_process: Vec<ChunkNode> = db.query(sql)
        .await.map_err(|e| e.to_string())?
        .take(0).map_err(|e| e.to_string())?;

    if chunks_to_process.is_empty() {
        return Ok("âœ¨ ì²˜ë¦¬í•  ìƒˆë¡œìš´ Chunkê°€ ì—†ìŠµë‹ˆë‹¤.".to_string());
    }

    let total = chunks_to_process.len();
    println!(" ğŸš€ Linking {} chunks based on tags/keywords...", total);

    let mut success_count = 0;

    for chunk in chunks_to_process.iter() {
        let chunk_thing = match &chunk.id {
            Some(t) => t.clone(),
            None => continue,
        };

        // 2. ë©”íƒ€ë°ì´í„°ì—ì„œ í‚¤ì›Œë“œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±°)
        let mut topics: HashSet<String> = HashSet::new();

        // ì°¸ê³ : analyze_contentì˜ ê²°ê³¼ê°€ metadata["analysis"]ì— ë“¤ì–´ìˆë‹¤ê³  ê°€ì •
        // ì§ì ‘ì ì¸ "tags"ë‚˜ "keywords" í•„ë“œê°€ ì—†ë‹¤ë©´ ì•„ë˜ ë¡œì§ì€ ë¹ˆ ë™ì‘ì„ í•  ìˆ˜ ìˆìŒ.
        // í•„ìš” ì‹œ chunk.metadata["analysis"]["key_entities"] ë“±ì„ íŒŒì‹±í•˜ë„ë¡ ìˆ˜ì • ê°€ëŠ¥.
        
        // (1) Tags íƒìƒ‰
        if let Some(tags_val) = chunk.metadata.get("tags") {
            if let Some(arr) = tags_val.as_array() {
                for t in arr {
                    if let Some(s) = t.as_str() { topics.insert(s.trim().to_string()); }
                }
            }
        }

        // (2) Keywords íƒìƒ‰
        if let Some(kws_val) = chunk.metadata.get("keywords") {
            if let Some(arr) = kws_val.as_array() {
                for k in arr {
                    if let Some(s) = k.as_str() { topics.insert(s.trim().to_string()); }
                }
            }
        }
        
        // (3) Analysis ê²°ê³¼ ë‚´ key_entities íƒìƒ‰ (ì¶”ê°€ ë³´ì™„)
        if let Some(analysis_val) = chunk.metadata.get("analysis") {
            if let Some(entities) = analysis_val.get("key_entities").and_then(|v| v.as_array()) {
                for e in entities {
                    if let Some(s) = e.as_str() { topics.insert(s.trim().to_string()); }
                }
            }
        }

        // 3. Entity ìƒì„± ë° ì—°ê²°
        for topic in topics {
            if topic.is_empty() { continue; }

            let safe_name = crate::utils::sanitize_id(&topic);
            let entity_id = Thing::from(("entity", safe_name.as_str()));

            // Entity Upsert
            let _: Option<EntityNode> = db
                .upsert(("entity", &safe_name))
                .content(EntityNode {
                    id: Some(entity_id.clone()),
                    name: topic.clone(),
                    category: "Keyword".to_string(),
                    description: format!("Extracted keyword: {}", topic),
                    embedding: vec![],
                    created_at: Utc::now(),
                })
                .await.ok().flatten();

            // Chunk -> mentions -> Entity ì—°ê²°
            let sql = "RELATE $c -> mentions -> $e";
            let _ = db.query(sql)
                .bind(("c", chunk_thing.clone()))
                .bind(("e", entity_id))
                .await.ok();
        }

        // 4. ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
        let _: Option<ChunkNode> = db.update(("chunk", chunk_thing.id.to_string()))
            .merge(json!({
                "metadata": { "step2_processed": true }
            }))
            .await.ok().flatten();

        success_count += 1;
    }

    Ok(format!("âœ… {}/{} ê°œì˜ ì²­í¬ ì—°ê²° ì™„ë£Œ (ê³ ì† ëª¨ë“œ)", success_count, total))
}

// --- ë¬¸ì„œ ì¡°íšŒ (ê³„ì¸µ êµ¬ì¡° í¬í•¨) ---
#[tauri::command]
pub async fn get_documents(state: State<'_, AppState>) -> Result<Vec<DocumentWithChunks>, String> {
    let db = &state.db;
    
    // ì„œë¸Œì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ Documentì™€ ì—°ê´€ëœ Chunkë“¤ì„ í•œ ë²ˆì— ì¡°íšŒ
    let sql = "
        SELECT 
            *, 
            (SELECT * FROM ->contains->chunk ORDER BY page_index ASC) AS chunks 
        FROM document 
        ORDER BY created_at DESC
    ";
    
    let mut response = db.query(sql).await.map_err(|e| e.to_string())?;
    let documents: Vec<DocumentWithChunks> = response.take(0).map_err(|e| e.to_string())?;
    
    Ok(documents)
}