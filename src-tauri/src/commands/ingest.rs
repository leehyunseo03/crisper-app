// src-tauri/src/commands/ingest.rs
use tauri::State;
use std::path::Path;
use std::fs;
use uuid::Uuid;
use chrono::Utc;
use surrealdb::engine::local::Db;
use surrealdb::Surreal;
use surrealdb::sql::{Thing, Id};
use rig::embeddings::EmbeddingsBuilder;
use rig::client::EmbeddingsClient;
use std::collections::HashMap;
use serde_json::json;
use std::time::Instant;
use serde::{Serialize, Deserialize};
use std::collections::HashSet;

use crate::models::{EventNode, DocumentNode, ChunkNode, EntityNode, LlmExtractionResult};
use crate::utils::sanitize_id;
use crate::utils::{extract_pages_from_pdf, chunk_text, RigDoc};
use crate::llm::extractor::{extract_knowledge, summarize_document};
use crate::AppState;
use crate::utils::parse_kakao_talk_log;

#[derive(Debug, Serialize, Deserialize)]
pub struct DocumentWithChunks {
    pub id: Thing,
    pub filename: String,
    pub created_at: chrono::DateTime<Utc>,
    pub metadata: HashMap<String, serde_json::Value>,
    // ğŸŒŸ ì—¬ê¸°ê°€ í•µì‹¬: SurrealDBê°€ ì—°ê²°ëœ ì²­í¬ë“¤ì„ ì´ í•„ë“œì— ì±„ì›Œì¤ë‹ˆë‹¤.
    #[serde(default)] 
    pub chunks: Vec<ChunkNode>, 
}

#[tauri::command]
pub async fn ingest_documents(
    path: String,
    state: State<'_, AppState>,
) -> Result<String, String> {
    let db = &state.db;
    let gen_url = "http://127.0.0.1:8081/v1";

    println!("\nğŸ“‚ [Step 1] Ingest Process Started (1 Page = 1 Chunk)");
    println!("   Target Directory: {}", path);

    // 1. íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘ (ê¸°ì¡´ ë™ì¼)
    let entries = fs::read_dir(&path).map_err(|e| e.to_string())?;
    let mut pdf_files = Vec::new();
    for entry in entries { /* ... */ 
        let entry = entry.map_err(|e| e.to_string())?;
        let path = entry.path();
        if path.extension().and_then(|s| s.to_str()) == Some("pdf") {
            pdf_files.push(path);
        }
    }
    
    let total_files = pdf_files.len();
    if total_files == 0 { return Err("No PDF files found.".to_string()); }

    // 2. ì„¸ì…˜ ìƒì„± (ê¸°ì¡´ ë™ì¼)
    let session_id = Uuid::new_v4().to_string();
    let _: EventNode = db.create(("event", &session_id))
        .content(EventNode {
            id: None, summary: format!("PDF Ingest: {}", path), created_at: Utc::now(),
        }).await.map_err(|e| e.to_string())?.ok_or("Event create failed")?;

    let mut success_count = 0;

    // 3. íŒŒì¼ ì²˜ë¦¬ ë£¨í”„
    for (idx, file_path) in pdf_files.iter().enumerate() {
        let current_num = idx + 1;
        let original_filename = file_path.file_name().unwrap().to_string_lossy().to_string();
        
        println!("\n---------------------------------------------------");
        println!("â–¶ï¸  [{}/{}] Processing: {}", current_num, total_files, original_filename);
        let file_start = Instant::now();

        // A. ğŸŒŸ [í•µì‹¬ ë³€ê²½] í˜ì´ì§€ë³„ í…ìŠ¤íŠ¸ ì¶”ì¶œ (Vec<String>)
        print!("    ğŸ“– Extracting pages... ");
        let pages = match extract_pages_from_pdf(file_path) {
            Ok(p) => {
                println!("Done ({} pages)", p.len());
                p
            },
            Err(e) => {
                println!("âŒ Failed: {}", e);
                continue;
            }
        };

        if pages.is_empty() { 
            println!("    âš ï¸ Skipped (Empty PDF)");
            continue; 
        }

        // B. Document(ë¶€ëª¨) ìš”ì•½ ìƒì„±
        // ì „ì²´ í…ìŠ¤íŠ¸ê°€ ì—†ìœ¼ë¯€ë¡œ, ì•ìª½ 1~2í˜ì´ì§€ë¥¼ í•©ì³ì„œ ë¶€ëª¨ ë¬¸ì„œì˜ ìš”ì•½ìš©ìœ¼ë¡œ ì”ë‹ˆë‹¤.
        let summary_context = pages.iter().take(2).cloned().collect::<Vec<String>>().join("\n");
        
        println!("    ğŸ¤– Summarizing Document (Parent)...");
        let parent_summary = summarize_document(gen_url, &summary_context).await.unwrap_or_else(|_| {
             crate::llm::extractor::DocSummaryResult {
                title: original_filename.clone(), summary: "Parent Summary Failed".to_string(), tags: vec![], keywords: vec![],
            }
        });

        // Document ì €ì¥
        let doc_id = Uuid::new_v4().to_string();
        let mut doc_meta = HashMap::new();
        doc_meta.insert("title".to_string(), json!(parent_summary.title));
        doc_meta.insert("summary".to_string(), json!(parent_summary.summary));
        
        let _doc: DocumentNode = db.create(("document", &doc_id))
            .content(DocumentNode { 
                id: None, filename: original_filename.clone(), created_at: Utc::now(), metadata: doc_meta 
            }).await.map_err(|e| e.to_string())?.expect("Failed to create doc");

        // Event ì—°ê²°
        let _ = db.query("RELATE $e->imported->$d").bind(("e", session_id.clone())).bind(("d", format!("document:{}", doc_id))).await.ok();

        // C. ì²­í‚¹ (ì´ë¯¸ í˜ì´ì§€ë³„ë¡œ ë‚˜ëˆ ì ¸ ìˆìœ¼ë¯€ë¡œ chunk_text í•¨ìˆ˜ í˜¸ì¶œ ì•ˆ í•¨!)
        // let chunks = chunk_text(...) -> ì‚­ì œ!
        // pages ë³€ìˆ˜ ìì²´ê°€ ì²­í¬ ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤.
        let chunks = pages; 

        println!("    Process {} Pages as Chunks...", chunks.len());

        // D. ê° í˜ì´ì§€ë³„ LLM ìš”ì•½ ì‹¤í–‰
        for (i, txt) in chunks.iter().enumerate() {
            let chunk_uuid = Uuid::new_v4().to_string();
            
            // í˜ì´ì§€ê°€ ë„ˆë¬´ ê¸¸ ìˆ˜ ìˆìœ¼ë‹ˆ ìš”ì•½ìš©ìœ¼ë¡œëŠ” ì•ë¶€ë¶„ë§Œ ìë¥¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.
            // ì—¬ê¸°ì„  ê·¸ëŒ€ë¡œ ë„£ìŠµë‹ˆë‹¤.
            print!("       Running LLM on Page #{}... ", i + 1);
            
            // í˜ì´ì§€ë³„ ìš”ì•½ (ì œëª©ì— í˜ì´ì§€ ë²ˆí˜¸ ìë™ ë¶€ì—¬)
            let chunk_res = summarize_document(gen_url, txt).await.unwrap_or_else(|_| {
                 crate::llm::extractor::DocSummaryResult {
                    title: format!("Page {}", i+1), // LLM ì‹¤íŒ¨ì‹œ "Page 1" ë“±ìœ¼ë¡œ ì œëª© ì„¤ì •
                    summary: "ìš”ì•½ ì‹¤íŒ¨".to_string(),
                    tags: vec![],
                    keywords: vec![]
                }
            });
            println!("Done");

            let mut chunk_meta = HashMap::new();
            chunk_meta.insert("title".to_string(), json!(chunk_res.title)); // "ì„œë¡ ", "ê²°ë¡ " ë“± í˜ì´ì§€ ë‚´ìš©ì„ ë°˜ì˜í•œ ì œëª©
            chunk_meta.insert("summary".to_string(), json!(chunk_res.summary));
            chunk_meta.insert("tags".to_string(), json!(chunk_res.tags));
            chunk_meta.insert("keywords".to_string(), json!(chunk_res.keywords));
            chunk_meta.insert("page_number".to_string(), json!(i + 1)); // ğŸŒŸ ëª‡ í˜ì´ì§€ì¸ì§€ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€
            
            // Chunk ì €ì¥
            let _chunk: ChunkNode = db.create(("chunk", &chunk_uuid))
                .content(ChunkNode {
                    id: None, 
                    content: txt.clone(), 
                    page_index: i, 
                    embedding: vec![],
                    metadata: chunk_meta 
                }).await.map_err(|e| e.to_string())?.expect("Chunk create failed");
            println!("       ----------------------------------------");
            println!("       ğŸ“„ Title:   {}", chunk_res.title);
            println!("       ğŸ“ Summary: {}", chunk_res.summary);
            println!("       ğŸ·ï¸ Tags:    {:?}", chunk_res.tags);
            println!("       ----------------------------------------");
            // Document -> Chunk ì—°ê²°
            let doc_thing = Thing::from(("document", doc_id.as_str()));
            let chunk_thing = Thing::from(("chunk", chunk_uuid.as_str()));

            db.query("RELATE $d->contains->$c")
                .bind(("d", doc_thing))
                .bind(("c", chunk_thing))
                .await
                .ok();
        }

        println!("    âœ¨ File completed in {:.2?}", file_start.elapsed());
        success_count += 1;
    }

    Ok(format!("âœ… Processed {} files.", success_count))
}
// --- 2ë‹¨ê³„: Document(Chunk) -> Graph (ì˜¤ë˜ ê±¸ë¦¼) ---
#[tauri::command]
pub async fn construct_graph(
    state: State<'_, AppState>,
) -> Result<String, String> {
    let db = &state.db;

    println!("\nğŸ•¸ï¸ [Step 2] Building Keyword Graph (No LLM)...");

    // 1. ì•„ì§ ì²˜ë¦¬ë˜ì§€ ì•Šì€ Chunk ì¡°íšŒ (í•œ ë²ˆì— 500ê°œë„ ê±°ëœ¬í•¨)
    let sql = "SELECT * FROM chunk WHERE metadata.step2_processed != true LIMIT 500";
    
    let mut chunks_to_process: Vec<ChunkNode> = db.query(sql)
        .await.map_err(|e| e.to_string())?
        .take(0).map_err(|e| e.to_string())?;

    if chunks_to_process.is_empty() {
        return Ok("âœ¨ ì²˜ë¦¬í•  ìƒˆë¡œìš´ Chunkê°€ ì—†ìŠµë‹ˆë‹¤.".to_string());
    }

    let total = chunks_to_process.len();
    println!(" ğŸš€ Linking {} chunks based on tags/keywords...", total);

    let mut success_count = 0;

    for chunk in chunks_to_process.iter() {
        let chunk_thing = match &chunk.id {
            Some(t) => t.clone(),
            None => continue,
        };

        // 2. ë©”íƒ€ë°ì´í„°ì—ì„œ íƒœê·¸ì™€ í‚¤ì›Œë“œ ìˆ˜ì§‘
        // ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ HashSet ì‚¬ìš©
        let mut topics: HashSet<String> = HashSet::new();

        // (1) Tags ê°€ì ¸ì˜¤ê¸°
        if let Some(tags_val) = chunk.metadata.get("tags") {
            if let Some(arr) = tags_val.as_array() {
                for t in arr {
                    if let Some(s) = t.as_str() {
                        topics.insert(s.trim().to_string());
                    }
                }
            }
        }

        // (2) Keywords ê°€ì ¸ì˜¤ê¸° (ì´ì „ ì§ˆë¬¸ì—ì„œ ì¶”ê°€í•œ í•„ë“œ)
        if let Some(kws_val) = chunk.metadata.get("keywords") {
            if let Some(arr) = kws_val.as_array() {
                for k in arr {
                    if let Some(s) = k.as_str() {
                        topics.insert(s.trim().to_string());
                    }
                }
            }
        }

        // 3. ê° í† í”½ì„ Entityë¡œ ë§Œë“¤ê³  ì—°ê²°í•˜ê¸°
        for topic in topics {
            if topic.is_empty() { continue; }

            let safe_name = crate::utils::sanitize_id(&topic); // IDìš©ìœ¼ë¡œ íŠ¹ìˆ˜ë¬¸ì ì œê±°
            let entity_id = Thing::from(("entity", safe_name.as_str()));

            // 3-1. Entity ìƒì„± (ë‹¨ìˆœ Upsert)
            // LLM ìš”ì•½ì´ ì—†ìœ¼ë¯€ë¡œ descriptionì€ topic ì´ë¦„ ê·¸ëŒ€ë¡œ ì”€
            let _: Option<EntityNode> = db
                .upsert(("entity", &safe_name))
                .content(EntityNode {
                    id: Some(entity_id.clone()),
                    name: topic.clone(),
                    category: "Keyword".to_string(), // ì¹´í…Œê³ ë¦¬ í†µì¼
                    description: format!("Extracted keyword: {}", topic),
                    embedding: vec![],
                    created_at: Utc::now(),
                })
                .await.ok().flatten();

            // 3-2. ì—°ê²° (Chunk -> mentions -> Entity)
            let sql = "RELATE $c -> mentions -> $e";
            let _ = db.query(sql)
                .bind(("c", chunk_thing.clone()))
                .bind(("e", entity_id))
                .await.ok();
        }

        // 4. ì²˜ë¦¬ ì™„ë£Œ ë§ˆí‚¹
        let _: Option<ChunkNode> = db.update(("chunk", chunk_thing.id.to_string()))
            .merge(json!({
                "metadata": { "step2_processed": true }
            }))
            .await.ok().flatten();

        success_count += 1;
    }

    Ok(format!("âœ… {}/{} ê°œì˜ ì²­í¬ ì—°ê²° ì™„ë£Œ (ê³ ì† ëª¨ë“œ)", success_count, total))
}


#[tauri::command]
pub async fn get_documents(state: State<'_, AppState>) -> Result<Vec<DocumentWithChunks>, String> {
    let db = &state.db;
    
    // ğŸŒŸ [ìˆ˜ì • í•µì‹¬] ì„œë¸Œì¿¼ë¦¬ë¥¼ ì‚¬ìš©í•´ ì—°ê²°ëœ ë°ì´í„°ë¥¼ ì¤‘ì²© êµ¬ì¡°ë¡œ ê°€ì ¸ì˜µë‹ˆë‹¤.
    // ì˜ë¯¸: "documentë¥¼ ê°€ì ¸ì˜¤ëŠ”ë°, 'chunks'ë¼ëŠ” í•„ë“œì—ëŠ” 
    //      ë‚˜(document)ì™€ 'contains'ë¡œ ì—°ê²°ëœ 'chunk'ë“¤ì„ í˜ì´ì§€ ìˆœì„œëŒ€ë¡œ ë‹´ì•„ë¼"
    let sql = "
        SELECT 
            *, 
            (SELECT * FROM ->contains->chunk ORDER BY page_index ASC) AS chunks 
        FROM document 
        ORDER BY created_at DESC
    ";
    
    // ì¿¼ë¦¬ ì‹¤í–‰
    let mut response = db.query(sql).await.map_err(|e| e.to_string())?;
    
    // ê²°ê³¼ë¥¼ ìƒˆë¡œ ë§Œë“  êµ¬ì¡°ì²´(DocumentWithChunks) ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
    let documents: Vec<DocumentWithChunks> = response.take(0).map_err(|e| e.to_string())?;
    
    Ok(documents)
}

async fn save_graph_data(
    db: &Surreal<Db>,
    chunk_id: &Thing, // ğŸŒŸ String ëŒ€ì‹  Thingì„ ì§ì ‘ ë°›ìŒ (ì•ˆì „í•¨)
    data: &LlmExtractionResult,
) -> Result<(), String> {
    
    // 1. Entities ì €ì¥ ë° Chunk -> Entity ì—°ê²°
    for entity in &data.entities {
        let safe_name = sanitize_id(&entity.name);
        
        // Entity ID ìƒì„± (entity:ì´ë¦„)
        let entity_id = Thing::from(("entity", safe_name.as_str()));

        // 1-1. Entity ë…¸ë“œ ìƒì„± (Upsert)
        let _: Option<EntityNode> = db
            .upsert(("entity", &safe_name))
            .content(EntityNode {
                id: Some(entity_id.clone()),
                name: entity.name.clone(),
                category: entity.category.clone(),
                description: entity.summary.clone(),
                embedding: vec![],
                created_at: Utc::now(),
            })
            .await
            .map_err(|e| format!("Entity Upsert Error: {}", e))?;

        // 1-2. Chunk -> mentions -> Entity ì—°ê²°
        // "ì´ ì²­í¬(ë¬¸ì„œ ì¡°ê°)ê°€ ì´ ì—”í‹°í‹°ë¥¼ ì–¸ê¸‰í–ˆë‹¤"
        let sql = "RELATE $c -> mentions -> $e";
        let _ = db.query(sql)
            .bind(("c", chunk_id.clone())) 
            .bind(("e", entity_id))
            .await
            .map_err(|e| format!("Relate Chunk-Entity Error: {}", e))?;
    }

    // 2. Relations (Entity -> Entity) ì €ì¥
    for rel in &data.relations {
        let head_safe = sanitize_id(&rel.head);
        let tail_safe = sanitize_id(&rel.tail);

        let head_thing = Thing::from(("entity", head_safe.as_str()));
        let tail_thing = Thing::from(("entity", tail_safe.as_str()));

        // ê´€ê³„ì˜ ì–‘ ë ë…¸ë“œê°€ ì¡´ì¬í•˜ë„ë¡ ë¹ˆ ê»ë°ê¸°ë¼ë„ ìƒì„± (ì´ë¯¸ ìˆìœ¼ë©´ ì´ë¦„ë§Œ ì—…ë°ì´íŠ¸)
        // ì´ëŠ” LLMì´ ì¶”ì¶œí•œ ê´€ê³„ì˜ ëŒ€ìƒì´ ìœ„ entity ë¦¬ìŠ¤íŠ¸ì— ì—†ì„ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
        let _ = db.query("UPDATE type::thing('entity', $id) SET name = $name RETURN NONE")
            .bind(("id", head_safe.clone())).bind(("name", rel.head.clone())).await;
        let _ = db.query("UPDATE type::thing('entity', $id) SET name = $name RETURN NONE")
            .bind(("id", tail_safe.clone())).bind(("name", rel.tail.clone())).await;

        // 2-1. Entity -> related_to -> Entity ì—°ê²°
        let sql = "
            RELATE $h -> related_to -> $t
            CONTENT {
                relation: $rel,
                reason: $reason,
                created_at: time::now()
            }
        ";
        
        let _ = db.query(sql)
            .bind(("h", head_thing))
            .bind(("t", tail_thing))
            .bind(("rel", rel.relation.clone()))
            .bind(("reason", rel.reason.clone()))
            .await
            .map_err(|e| format!("Relate Entity-Entity Error: {}", e))?;
    }

    Ok(())
}

